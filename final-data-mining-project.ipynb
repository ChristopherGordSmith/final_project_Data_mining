{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Importing all necessary libraries\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.layers import Activation, Dropout, Flatten, Dense\nfrom keras import backend as K\nimport numpy as np\nimport pandas as pd\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-11-30T03:16:39.738962Z","iopub.execute_input":"2022-11-30T03:16:39.739436Z","iopub.status.idle":"2022-11-30T03:16:39.746538Z","shell.execute_reply.started":"2022-11-30T03:16:39.739401Z","shell.execute_reply":"2022-11-30T03:16:39.745211Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# my code\ntest_total = 0\nvalid_total = 0\ntrain_total = 0\n#test count\nfor image in os.listdir(\"/kaggle/input/seperateddataset/AnimalClassifyData/TestData/cheetah\"):\n    if image.endswith(\".png\"):\n        test_total = test_total + 1\nfor image in os.listdir(\"/kaggle/input/seperateddataset/AnimalClassifyData/TestData/fox\"):\n    if image.endswith(\".png\"):\n        test_total = test_total + 1\nfor image in os.listdir(\"/kaggle/input/seperateddataset/AnimalClassifyData/TestData/hyena\"):\n    if image.endswith(\".png\"):\n        test_total = test_total + 1\nfor image in os.listdir(\"/kaggle/input/seperateddataset/AnimalClassifyData/TestData/lion\"):\n    if image.endswith(\".png\"):\n        test_total = test_total + 1\nfor image in os.listdir(\"/kaggle/input/seperateddataset/AnimalClassifyData/TestData/tiger\"):\n    if image.endswith(\".png\"):\n        test_total = test_total + 1\nfor image in os.listdir(\"/kaggle/input/seperateddataset/AnimalClassifyData/TestData/wolf\"):\n    if image.endswith(\".png\"):\n        test_total = test_total + 1\n#training count\nfor image in os.listdir(\"/kaggle/input/seperateddataset/AnimalClassifyData/TrainingData/cheetah\"):\n    if image.endswith(\".png\"):\n        train_total = train_total + 1\nfor image in os.listdir(\"/kaggle/input/seperateddataset/AnimalClassifyData/TrainingData/fox\"):\n    if image.endswith(\".png\"):\n        train_total = train_total + 1\nfor image in os.listdir(\"/kaggle/input/seperateddataset/AnimalClassifyData/TrainingData/hyena\"):\n    if image.endswith(\".png\"):\n        train_total = train_total + 1\nfor image in os.listdir(\"/kaggle/input/seperateddataset/AnimalClassifyData/TrainingData/lion\"):\n    if image.endswith(\".png\"):\n        train_total = train_total + 1\nfor image in os.listdir(\"/kaggle/input/seperateddataset/AnimalClassifyData/TrainingData/tiger\"):\n    if image.endswith(\".png\"):\n        train_total = train_total + 1\nfor image in os.listdir(\"/kaggle/input/seperateddataset/AnimalClassifyData/TrainingData/wolf\"):\n    if image.endswith(\".png\"):\n        train_total = train_total + 1\n#validation count\nfor image in os.listdir(\"/kaggle/input/seperateddataset/AnimalClassifyData/ValidationData/cheetah\"):\n    if image.endswith(\".png\"):\n        valid_total = valid_total + 1\nfor image in os.listdir(\"/kaggle/input/seperateddataset/AnimalClassifyData/ValidationData/fox\"):\n    if image.endswith(\".png\"):\n        valid_total = valid_total + 1\nfor image in os.listdir(\"/kaggle/input/seperateddataset/AnimalClassifyData/ValidationData/hyena\"):\n    if image.endswith(\".png\"):\n        valid_total = valid_total + 1\nfor image in os.listdir(\"/kaggle/input/seperateddataset/AnimalClassifyData/ValidationData/lion\"):\n    if image.endswith(\".png\"):\n        valid_total = valid_total + 1\nfor image in os.listdir(\"/kaggle/input/seperateddataset/AnimalClassifyData/ValidationData/tiger\"):\n    if image.endswith(\".png\"):\n        valid_total = valid_total + 1\nfor image in os.listdir(\"/kaggle/input/seperateddataset/AnimalClassifyData/ValidationData/wolf\"):\n    if image.endswith(\".png\"):\n        valid_total = valid_total + 1\nprint(\"Test total: \", test_total, \"Train total: \", train_total, \"Valid total: \", valid_total)\n#mycode","metadata":{"execution":{"iopub.status.busy":"2022-11-30T03:16:39.764763Z","iopub.execute_input":"2022-11-30T03:16:39.765172Z","iopub.status.idle":"2022-11-30T03:16:39.801351Z","shell.execute_reply.started":"2022-11-30T03:16:39.765139Z","shell.execute_reply":"2022-11-30T03:16:39.800325Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Test total:  341 Train total:  1035 Valid total:  347\n","output_type":"stream"}]},{"cell_type":"code","source":"#Geeks for Geeks article\nimg_width, img_height = 512, 512\ntrain_data_dir = '/kaggle/input/seperateddataset/AnimalClassifyData/TrainingData'\ntest_data_dir = '/kaggle/input/seperateddataset/AnimalClassifyData/TestData'\nvalidation_data_dir = '/kaggle/input/seperateddataset/AnimalClassifyData/ValidationData'\nepochs = 60\nbatch_size = 16\n#Geeks for Geeks article\n#My code, but it's the geeks for geeks kind of modified\nif K.image_data_format() == 'channels_first':\n    input_shape = (3, img_width, img_height)\nelse:\n    input_shape = (img_width, img_height, 3)\nmodel = Sequential() \nmodel.add(Conv2D(64, (3, 3), input_shape=input_shape))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(3, 3)))\n\nmodel.add(Conv2D(64, (3, 3)))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(3, 3)))\n  \nmodel.add(Conv2D(64, (3, 3)))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(3, 3)))\n  \nmodel.add(Flatten())\nmodel.add(Dense(64))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(6))\nmodel.add(Activation('softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['categorical_crossentropy'])\nmodel.summary()\n#My code, but it's the geeks for geeks kind of modified\n# medium tutorial all of it\ntrain_datagen = ImageDataGenerator(\n    rescale=1. / 255,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True)\n\nvalid_datagen = ImageDataGenerator(\n    rescale=1. / 255,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True)\n\ntest_datagen = ImageDataGenerator(rescale=1. / 255)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_data_dir,\n    target_size=(img_width, img_height),\n    batch_size=batch_size,\n    class_mode='categorical',\n    color_mode=\"rgb\",\n    shuffle=True)\n  \nvalidation_generator = valid_datagen.flow_from_directory(\n    validation_data_dir,\n    target_size=(img_width, img_height),\n    batch_size=batch_size,\n    class_mode='categorical',\n    color_mode=\"rgb\",\n    shuffle=True)\n\ntest_generator = test_datagen.flow_from_directory(\n    test_data_dir,\n    target_size=(img_width, img_height),\n    batch_size=1,\n    class_mode=None,\n    color_mode=\"rgb\",\n    shuffle=False\n)\n\nmodel.fit(train_generator,steps_per_epoch=train_total//batch_size,epochs=epochs,validation_data=validation_generator,validation_steps=valid_total//batch_size)\n# medium tutorial all of it\n# medium tutorial\nstep_size = test_generator.n//test_generator.batch_size\ntest_generator.reset()\npred_class_indices = np.argmax(model.predict(test_generator,steps=step_size, verbose=1), axis=-1)\n# medium tutorial all\n# medium tutorial\nlabels = (train_generator.class_indices)\nlabels = dict((v,k) for k,v in labels.items())\npredictions = [labels[k] for k in pred_class_indices]\nfilenames=test_generator.filenames\n# medium tutorial\n#my code\ncorrect = 0\nfor i in range(341):\n    if not filenames[i].find(predictions[i]):\n        correct = correct + 1\nprint(\"Accuracy: %\", (correct/341)*100)\n# #my code","metadata":{"execution":{"iopub.status.busy":"2022-11-30T03:16:39.803090Z","iopub.execute_input":"2022-11-30T03:16:39.803607Z","iopub.status.idle":"2022-11-30T07:09:33.695230Z","shell.execute_reply.started":"2022-11-30T03:16:39.803575Z","shell.execute_reply":"2022-11-30T07:09:33.694200Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Model: \"sequential_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_9 (Conv2D)            (None, 510, 510, 64)      1792      \n_________________________________________________________________\nactivation_15 (Activation)   (None, 510, 510, 64)      0         \n_________________________________________________________________\nmax_pooling2d_9 (MaxPooling2 (None, 170, 170, 64)      0         \n_________________________________________________________________\nconv2d_10 (Conv2D)           (None, 168, 168, 64)      36928     \n_________________________________________________________________\nactivation_16 (Activation)   (None, 168, 168, 64)      0         \n_________________________________________________________________\nmax_pooling2d_10 (MaxPooling (None, 56, 56, 64)        0         \n_________________________________________________________________\nconv2d_11 (Conv2D)           (None, 54, 54, 64)        36928     \n_________________________________________________________________\nactivation_17 (Activation)   (None, 54, 54, 64)        0         \n_________________________________________________________________\nmax_pooling2d_11 (MaxPooling (None, 18, 18, 64)        0         \n_________________________________________________________________\nflatten_3 (Flatten)          (None, 20736)             0         \n_________________________________________________________________\ndense_6 (Dense)              (None, 64)                1327168   \n_________________________________________________________________\nactivation_18 (Activation)   (None, 64)                0         \n_________________________________________________________________\ndropout_3 (Dropout)          (None, 64)                0         \n_________________________________________________________________\ndense_7 (Dense)              (None, 6)                 390       \n_________________________________________________________________\nactivation_19 (Activation)   (None, 6)                 0         \n=================================================================\nTotal params: 1,403,206\nTrainable params: 1,403,206\nNon-trainable params: 0\n_________________________________________________________________\nFound 1035 images belonging to 6 classes.\nFound 347 images belonging to 6 classes.\nFound 341 images belonging to 6 classes.\nEpoch 1/60\n64/64 [==============================] - 230s 4s/step - loss: 1.7923 - categorical_crossentropy: 1.7923 - val_loss: 1.7819 - val_categorical_crossentropy: 1.7819\nEpoch 2/60\n64/64 [==============================] - 228s 4s/step - loss: 1.7849 - categorical_crossentropy: 1.7849 - val_loss: 1.7753 - val_categorical_crossentropy: 1.7753\nEpoch 3/60\n64/64 [==============================] - 228s 4s/step - loss: 1.7737 - categorical_crossentropy: 1.7737 - val_loss: 1.7609 - val_categorical_crossentropy: 1.7609\nEpoch 4/60\n64/64 [==============================] - 226s 4s/step - loss: 1.7519 - categorical_crossentropy: 1.7519 - val_loss: 1.7285 - val_categorical_crossentropy: 1.7285\nEpoch 5/60\n64/64 [==============================] - 223s 3s/step - loss: 1.7403 - categorical_crossentropy: 1.7403 - val_loss: 1.7179 - val_categorical_crossentropy: 1.7179\nEpoch 6/60\n64/64 [==============================] - 222s 3s/step - loss: 1.7059 - categorical_crossentropy: 1.7059 - val_loss: 1.6789 - val_categorical_crossentropy: 1.6789\nEpoch 7/60\n64/64 [==============================] - 220s 3s/step - loss: 1.6987 - categorical_crossentropy: 1.6987 - val_loss: 1.6853 - val_categorical_crossentropy: 1.6853\nEpoch 8/60\n64/64 [==============================] - 216s 3s/step - loss: 1.6569 - categorical_crossentropy: 1.6569 - val_loss: 1.6279 - val_categorical_crossentropy: 1.6279\nEpoch 9/60\n64/64 [==============================] - 217s 3s/step - loss: 1.6234 - categorical_crossentropy: 1.6234 - val_loss: 1.5694 - val_categorical_crossentropy: 1.5694\nEpoch 10/60\n64/64 [==============================] - 216s 3s/step - loss: 1.6031 - categorical_crossentropy: 1.6031 - val_loss: 1.5566 - val_categorical_crossentropy: 1.5566\nEpoch 11/60\n64/64 [==============================] - 214s 3s/step - loss: 1.5889 - categorical_crossentropy: 1.5889 - val_loss: 1.5241 - val_categorical_crossentropy: 1.5241\nEpoch 12/60\n64/64 [==============================] - 216s 3s/step - loss: 1.5442 - categorical_crossentropy: 1.5442 - val_loss: 1.4755 - val_categorical_crossentropy: 1.4755\nEpoch 13/60\n64/64 [==============================] - 216s 3s/step - loss: 1.4890 - categorical_crossentropy: 1.4890 - val_loss: 1.4258 - val_categorical_crossentropy: 1.4258\nEpoch 14/60\n64/64 [==============================] - 215s 3s/step - loss: 1.4345 - categorical_crossentropy: 1.4345 - val_loss: 1.4041 - val_categorical_crossentropy: 1.4041\nEpoch 15/60\n64/64 [==============================] - 218s 3s/step - loss: 1.4055 - categorical_crossentropy: 1.4055 - val_loss: 1.3383 - val_categorical_crossentropy: 1.3383\nEpoch 16/60\n64/64 [==============================] - 221s 3s/step - loss: 1.3781 - categorical_crossentropy: 1.3781 - val_loss: 1.4206 - val_categorical_crossentropy: 1.4206\nEpoch 17/60\n64/64 [==============================] - 217s 3s/step - loss: 1.3130 - categorical_crossentropy: 1.3130 - val_loss: 1.2916 - val_categorical_crossentropy: 1.2916\nEpoch 18/60\n64/64 [==============================] - 218s 3s/step - loss: 1.3428 - categorical_crossentropy: 1.3428 - val_loss: 1.2561 - val_categorical_crossentropy: 1.2561\nEpoch 19/60\n64/64 [==============================] - 217s 3s/step - loss: 1.2551 - categorical_crossentropy: 1.2551 - val_loss: 1.2974 - val_categorical_crossentropy: 1.2974\nEpoch 20/60\n64/64 [==============================] - 217s 3s/step - loss: 1.2472 - categorical_crossentropy: 1.2472 - val_loss: 1.2522 - val_categorical_crossentropy: 1.2522\nEpoch 21/60\n64/64 [==============================] - 217s 3s/step - loss: 1.2090 - categorical_crossentropy: 1.2090 - val_loss: 1.2377 - val_categorical_crossentropy: 1.2377\nEpoch 22/60\n64/64 [==============================] - 216s 3s/step - loss: 1.2095 - categorical_crossentropy: 1.2095 - val_loss: 1.2596 - val_categorical_crossentropy: 1.2596\nEpoch 23/60\n64/64 [==============================] - 217s 3s/step - loss: 1.1769 - categorical_crossentropy: 1.1769 - val_loss: 1.2016 - val_categorical_crossentropy: 1.2016\nEpoch 24/60\n64/64 [==============================] - 215s 3s/step - loss: 1.1379 - categorical_crossentropy: 1.1379 - val_loss: 1.1617 - val_categorical_crossentropy: 1.1617\nEpoch 25/60\n64/64 [==============================] - 216s 3s/step - loss: 1.1085 - categorical_crossentropy: 1.1085 - val_loss: 1.1708 - val_categorical_crossentropy: 1.1708\nEpoch 26/60\n64/64 [==============================] - 215s 3s/step - loss: 1.1156 - categorical_crossentropy: 1.1156 - val_loss: 1.2292 - val_categorical_crossentropy: 1.2292\nEpoch 27/60\n64/64 [==============================] - 216s 3s/step - loss: 1.0793 - categorical_crossentropy: 1.0793 - val_loss: 1.1973 - val_categorical_crossentropy: 1.1973\nEpoch 28/60\n64/64 [==============================] - 216s 3s/step - loss: 1.0469 - categorical_crossentropy: 1.0469 - val_loss: 1.1705 - val_categorical_crossentropy: 1.1705\nEpoch 29/60\n64/64 [==============================] - 215s 3s/step - loss: 1.0551 - categorical_crossentropy: 1.0551 - val_loss: 1.1579 - val_categorical_crossentropy: 1.1579\nEpoch 30/60\n64/64 [==============================] - 217s 3s/step - loss: 1.0035 - categorical_crossentropy: 1.0035 - val_loss: 1.1456 - val_categorical_crossentropy: 1.1456\nEpoch 31/60\n64/64 [==============================] - 216s 3s/step - loss: 1.0343 - categorical_crossentropy: 1.0343 - val_loss: 1.1187 - val_categorical_crossentropy: 1.1187\nEpoch 32/60\n64/64 [==============================] - 216s 3s/step - loss: 0.9431 - categorical_crossentropy: 0.9431 - val_loss: 1.2769 - val_categorical_crossentropy: 1.2769\nEpoch 33/60\n64/64 [==============================] - 217s 3s/step - loss: 1.0012 - categorical_crossentropy: 1.0012 - val_loss: 1.1179 - val_categorical_crossentropy: 1.1179\nEpoch 34/60\n64/64 [==============================] - 215s 3s/step - loss: 0.9500 - categorical_crossentropy: 0.9500 - val_loss: 1.1697 - val_categorical_crossentropy: 1.1697\nEpoch 35/60\n64/64 [==============================] - 216s 3s/step - loss: 0.9541 - categorical_crossentropy: 0.9541 - val_loss: 1.1054 - val_categorical_crossentropy: 1.1054\nEpoch 36/60\n64/64 [==============================] - 217s 3s/step - loss: 0.9529 - categorical_crossentropy: 0.9529 - val_loss: 1.1264 - val_categorical_crossentropy: 1.1264\nEpoch 37/60\n64/64 [==============================] - 218s 3s/step - loss: 0.8996 - categorical_crossentropy: 0.8996 - val_loss: 1.1173 - val_categorical_crossentropy: 1.1173\nEpoch 38/60\n64/64 [==============================] - 218s 3s/step - loss: 0.9308 - categorical_crossentropy: 0.9308 - val_loss: 1.0333 - val_categorical_crossentropy: 1.0333\nEpoch 39/60\n64/64 [==============================] - 221s 3s/step - loss: 0.9138 - categorical_crossentropy: 0.9138 - val_loss: 1.1221 - val_categorical_crossentropy: 1.1221\nEpoch 40/60\n64/64 [==============================] - 221s 3s/step - loss: 0.8664 - categorical_crossentropy: 0.8664 - val_loss: 1.0773 - val_categorical_crossentropy: 1.0773\nEpoch 41/60\n64/64 [==============================] - 220s 3s/step - loss: 0.8578 - categorical_crossentropy: 0.8578 - val_loss: 1.0824 - val_categorical_crossentropy: 1.0824\nEpoch 42/60\n64/64 [==============================] - 218s 3s/step - loss: 0.8332 - categorical_crossentropy: 0.8332 - val_loss: 1.1860 - val_categorical_crossentropy: 1.1860\nEpoch 43/60\n64/64 [==============================] - 219s 3s/step - loss: 0.8437 - categorical_crossentropy: 0.8437 - val_loss: 1.1131 - val_categorical_crossentropy: 1.1131\nEpoch 44/60\n64/64 [==============================] - 217s 3s/step - loss: 0.8187 - categorical_crossentropy: 0.8187 - val_loss: 1.1014 - val_categorical_crossentropy: 1.1014\nEpoch 45/60\n64/64 [==============================] - 221s 3s/step - loss: 0.8074 - categorical_crossentropy: 0.8074 - val_loss: 1.0631 - val_categorical_crossentropy: 1.0631\nEpoch 46/60\n64/64 [==============================] - 217s 3s/step - loss: 0.8044 - categorical_crossentropy: 0.8044 - val_loss: 1.0874 - val_categorical_crossentropy: 1.0874\nEpoch 47/60\n64/64 [==============================] - 216s 3s/step - loss: 0.7402 - categorical_crossentropy: 0.7402 - val_loss: 1.2384 - val_categorical_crossentropy: 1.2384\nEpoch 48/60\n64/64 [==============================] - 218s 3s/step - loss: 0.7937 - categorical_crossentropy: 0.7937 - val_loss: 1.0351 - val_categorical_crossentropy: 1.0351\nEpoch 49/60\n64/64 [==============================] - 216s 3s/step - loss: 0.7333 - categorical_crossentropy: 0.7333 - val_loss: 1.0894 - val_categorical_crossentropy: 1.0894\nEpoch 50/60\n64/64 [==============================] - 217s 3s/step - loss: 0.7261 - categorical_crossentropy: 0.7261 - val_loss: 1.0643 - val_categorical_crossentropy: 1.0643\nEpoch 51/60\n64/64 [==============================] - 224s 3s/step - loss: 0.7338 - categorical_crossentropy: 0.7338 - val_loss: 1.0914 - val_categorical_crossentropy: 1.0914\nEpoch 52/60\n64/64 [==============================] - 217s 3s/step - loss: 0.7148 - categorical_crossentropy: 0.7148 - val_loss: 1.0962 - val_categorical_crossentropy: 1.0962\nEpoch 53/60\n64/64 [==============================] - 215s 3s/step - loss: 0.7043 - categorical_crossentropy: 0.7043 - val_loss: 1.0914 - val_categorical_crossentropy: 1.0914\nEpoch 54/60\n64/64 [==============================] - 220s 3s/step - loss: 0.7371 - categorical_crossentropy: 0.7371 - val_loss: 1.0432 - val_categorical_crossentropy: 1.0432\nEpoch 55/60\n64/64 [==============================] - 219s 3s/step - loss: 0.7227 - categorical_crossentropy: 0.7227 - val_loss: 1.1081 - val_categorical_crossentropy: 1.1081\nEpoch 56/60\n64/64 [==============================] - 221s 3s/step - loss: 0.6947 - categorical_crossentropy: 0.6947 - val_loss: 1.0158 - val_categorical_crossentropy: 1.0158\nEpoch 57/60\n64/64 [==============================] - 221s 3s/step - loss: 0.6822 - categorical_crossentropy: 0.6822 - val_loss: 1.0500 - val_categorical_crossentropy: 1.0500\nEpoch 58/60\n64/64 [==============================] - 221s 3s/step - loss: 0.6522 - categorical_crossentropy: 0.6522 - val_loss: 1.0315 - val_categorical_crossentropy: 1.0315\nEpoch 59/60\n64/64 [==============================] - 227s 4s/step - loss: 0.6483 - categorical_crossentropy: 0.6483 - val_loss: 1.1338 - val_categorical_crossentropy: 1.1338\nEpoch 60/60\n64/64 [==============================] - 222s 3s/step - loss: 0.6443 - categorical_crossentropy: 0.6443 - val_loss: 1.0733 - val_categorical_crossentropy: 1.0733\n341/341 [==============================] - 15s 44ms/step\nAccuracy: % 59.530791788856305\n","output_type":"stream"}]}]}